<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chinglish Small Talk</title>
  
  <subtitle>Tech and Personal Blog of Ofey Chan. Personal articles are in *Archives*.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-18T02:22:55.336Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ofey Chan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用 RSS Hub 自建 RSS 订阅</title>
    <link href="http://yoursite.com/2019/07/18/my-rss-hub/"/>
    <id>http://yoursite.com/2019/07/18/my-rss-hub/</id>
    <published>2019-07-18T02:18:36.000Z</published>
    <updated>2019-07-18T02:22:55.336Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>日常手册/指北 分享</p></blockquote><h2 id="综述和背景"><a href="#综述和背景" class="headerlink" title="综述和背景"></a>综述和背景</h2><p>RSS Hub 是一个活跃开发的开源项目，「万物皆可 RSS」的口号表明了它是一个能把各种乱七八糟的东西制作成 RSS feed，包括 twitter、微信公众号……的项目。</p><p>正好最近入手了 Reeder 4，同时又有通过 RSS 订阅 Issue 的需求，所以就复活自己在 DigitalOcean 上吃灰的云主机，来建他一个。</p><a id="more"></a><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><blockquote><p>解决办法适用的环境/版本</p></blockquote><ul><li>RSS Hub <a href="https://hub.docker.com/r/diygod/rsshub" target="_blank" rel="noopener">Docker 版本</a></li><li>DigitalOcean 云主机一只</li></ul><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><blockquote><p>逐步说明用什么工具, 在哪儿, 进行什么操作, 如何检验, 应该获得什么输出 …</p></blockquote><h3 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><p>使用 Docker 的方式来部署 RSS Hub，这是<a href="https://docs.rsshub.app/install/#%E5%AE%89%E8%A3%85-2" target="_blank" rel="noopener">官方手册</a>上的推荐方案之一。</p><p>主要还是 Docker 省心。</p><p>DO 竟然自带<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04" target="_blank" rel="noopener">安装 Docker 的说明</a>。省心程度+1</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install apt-transport-https ca-certificates curl software-properties-common</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo add-apt-repository <span class="string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"</span></span><br><span class="line"></span><br><span class="line">sudo apt update</span><br><span class="line">apt-cache policy docker-ce</span><br><span class="line">sudo apt install docker-ce</span><br><span class="line"></span><br><span class="line">sudo systemctl status docker</span><br></pre></td></tr></table></figure><p>DO 的该说明竟然连 Docker 命令的使用、Use docker without sudo 都讲到了，简直是入门手册……配套服务真的好。</p><h3 id="使用-Docker-部署"><a href="#使用-Docker-部署" class="headerlink" title="使用 Docker 部署"></a>使用 Docker 部署</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull diygod/rsshub</span><br><span class="line">docker run -d --name rsshub -p 1200:1200 diygod/rsshub</span><br></pre></td></tr></table></figure><p>访问 (ip of VPS):1200 可以看到一个光秃秃的界面。</p><h3 id="添加订阅源"><a href="#添加订阅源" class="headerlink" title="添加订阅源"></a>添加订阅源</h3><p><a href="https://docs.rsshub.app" target="_blank" rel="noopener">官方手册</a>的「路由」页面放了许多写好的路由规则，直接按照规则订阅就完事了。</p><p>自己写规则贡献规则什么的以后再说吧……说着就冲动消费入了个 Reeder 4 for iOS。</p><p>对某个源生成订阅的方法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://(your RSS Hub VPS ip):1200/路由</span><br><span class="line"><span class="comment"># eg:</span></span><br><span class="line">http://(your RSS Hub VPS ip):1200/nytimes/dual  <span class="comment"># 纽约时报</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p>再次说明经验应用要注意的, 容易出问题的点, 以及有助记忆的作弊条…</p></blockquote><p>真的很简单，没有什么出问题的点，一个坑都没踩到，二十分钟弄完了。</p><p>主要还是要找到最官方的手册，比如 DO 自己部署 Docker 的说明，和 RSS Hub 的官方手册，这样可以最大程度地避免踩到特殊环境的坑。</p><p>订阅生成法：<code>http://(your RSS Hub VPS ip):1200/路由</code></p><p>路由在<a href="https://docs.rsshub.app" target="_blank" rel="noopener">官方手册</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><blockquote><p>过程中参考过的重要文章/图书/模块/代码/…</p></blockquote><ul><li><a href="http://diygod.me/ohmyrss/" target="_blank" rel="noopener">我有特别的 RSS 技巧-DIYGOD</a></li><li><a href="https://sspai.com/post/41302" target="_blank" rel="noopener">如何搭建属于自己的 RSS 服务，高效精准获取信息-少数派</a></li><li><a href="https://docs.rsshub.app" target="_blank" rel="noopener">RSS Hub 官方手册</a></li><li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04" target="_blank" rel="noopener">DO 对安装 docker 的说明</a></li><li>永远的: <a href="https://gitlab.com/101camp/2py/tasks/wikis/HandBooks/Hb4Ask" target="_blank" rel="noopener">如何提问</a></li></ul><h2 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h2><ul><li>190718 ofey init</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;日常手册/指北 分享&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;综述和背景&quot;&gt;&lt;a href=&quot;#综述和背景&quot; class=&quot;headerlink&quot; title=&quot;综述和背景&quot;&gt;&lt;/a&gt;综述和背景&lt;/h2&gt;&lt;p&gt;RSS Hub 是一个活跃开发的开源项目，「万物皆可 RSS」的口号表明了它是一个能把各种乱七八糟的东西制作成 RSS feed，包括 twitter、微信公众号……的项目。&lt;/p&gt;
&lt;p&gt;正好最近入手了 Reeder 4，同时又有通过 RSS 订阅 Issue 的需求，所以就复活自己在 DigitalOcean 上吃灰的云主机，来建他一个。&lt;/p&gt;
    
    </summary>
    
      <category term="Digital Life" scheme="http://yoursite.com/categories/Digital-Life/"/>
    
    
      <category term="lan-chinese" scheme="http://yoursite.com/tags/lan-chinese/"/>
    
      <category term="reading" scheme="http://yoursite.com/tags/reading/"/>
    
      <category term="rss" scheme="http://yoursite.com/tags/rss/"/>
    
      <category term="productivity" scheme="http://yoursite.com/tags/productivity/"/>
    
      <category term="handbook" scheme="http://yoursite.com/tags/handbook/"/>
    
  </entry>
  
  <entry>
    <title>COP CRAFT 动画版——超越原作</title>
    <link href="http://yoursite.com/2019/07/17/cop-craft-anime/"/>
    <id>http://yoursite.com/2019/07/17/cop-craft-anime/</id>
    <published>2019-07-17T12:31:56.000Z</published>
    <updated>2019-07-17T12:36:51.808Z</updated>
    
    <content type="html"><![CDATA[<p>动画兼有满满的「贺东味」——不做花瓶遇事不会嘤嘤嘤的硬派女主、动作片式的场景调度与氛围把握、军宅对细节的考究……和目前看来相当不错的制作素质（与经费）。</p><a id="more"></a><p>看了一点 COP CRAFT 的小说，不得不说动画改得好。例如：</p><ol><li>将的场死去同事的年龄改大。留下妻子和未成年儿女，接到消息孩子们还不知道发生了什么。这比小说「订婚的未婚妻」冲击力大很多。</li><li>接到缇拉娜之后「服务读者」桥段的删减，乃至对异世界风貌叙述的整体压缩·，把重点完全放到犯罪之城的塑造和刑警-骑士搭档的磨合上。不破坏前两集灰暗且相当写实的警匪片气氛，前军人大叔警官和异世界（中世纪）人外萝莉剑士的关系培养的描写可以放到第一个事件结束，正好78福利回的时候。异世界的描述也可能是移了位置，之后再补完。</li></ol><p>序幕里桂警官和同事关于猫的对话改了，这是很有趣的一点。原本是没有性暗示的，是「接近会让彼此受伤」这种怨侣式笑话，动画中改成了「蓝眼睛的美女，每晚都在我背上留下抓痕」。</p><p>如果是因什么规定自主规制的话，应该是不会往成人的方向改的。我猜这可能是贺东的意思——毕竟他对自己的作品改的剧本一向非常上心，即使在 FMP 4 那种经费紧张的情况下也自己介入，保证了质量。</p><p>这点修改和之前增大死去同事的年龄达到了相似的效果，营造出了一种和轻松的普通动漫不同的，沉重的「 B 级警匪片」的质感。</p><p>要我评价的话，动漫的水平（单看前两集），是超越小说的。小说是贺东在处女作 FMP 之后的第二个作品，既有人物关系等细节上和前作的相似，也有一些「服务读者」的风格转变，在他的创作史上应该算比较青涩。动画版有机会补完这些遗憾很不错。</p><p>剧本有小说做底，贺东坐镇，出不了太大问题。只要制作不崩，基本可以放心食用。</p><p>话说在某个地方看到了第一版的非村田莲尔版插画，一言难尽……动画在人设观感方面的亮眼，真得感谢村田老师的人设。村田老师牛逼！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;动画兼有满满的「贺东味」——不做花瓶遇事不会嘤嘤嘤的硬派女主、动作片式的场景调度与氛围把握、军宅对细节的考究……和目前看来相当不错的制作素质（与经费）。&lt;/p&gt;
    
    </summary>
    
      <category term="ACG" scheme="http://yoursite.com/categories/ACG/"/>
    
      <category term="anime review" scheme="http://yoursite.com/categories/ACG/anime-review/"/>
    
    
      <category term="lan-chinese" scheme="http://yoursite.com/tags/lan-chinese/"/>
    
      <category term="cop craft" scheme="http://yoursite.com/tags/cop-craft/"/>
    
  </entry>
  
  <entry>
    <title>Issue 管理入门指北</title>
    <link href="http://yoursite.com/2019/07/16/introduction-to-issue-management/"/>
    <id>http://yoursite.com/2019/07/16/introduction-to-issue-management/</id>
    <published>2019-07-16T07:34:36.000Z</published>
    <updated>2019-07-16T07:37:21.866Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Consistent use of an issue or bug tracking system is considered one of the “hallmarks of a good software team”. – wikipedia</p></blockquote><p>作为社会化软件开发的必备技能，本文分享一些 Issue 管理的常识。</p><a id="more"></a><p>以下皆为本人阅读积累和逻辑分析，结论是保持开放的。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><blockquote><p>问题产生场景和激发隐藏任务事件/链接/…</p></blockquote><p>本文来自微信群中和大妈的一段对话：</p><blockquote><p>【大妈】：<br>在俺的直觉中:<br>0: Issue 根据性质/内容/目标不同, 有不同生命周期<br>1: 为了提醒大家和自己,在标题中注明是最好的<br>比如: 2d[FAQ] 如何下载git?<br>2: 关闭, 并不代表死亡..只是为了保证 Issue 首页能看到所有活跃提案, 甚至于, 先关闭, 自己完善到一定程度, 再打开公示也可以</p></blockquote><p>camp2py 学员们的信息交流、问题讨论，主要使用 <a href="https://gitlab.com/101camp/2py/tasks/issues" target="_blank" rel="noopener">task 仓库的 Issue 页面</a>。只要参与课程讨论，一定会使用 Issue。可预见将来参与正规的软件开发也会如此，Issue 的管理是合作软件开发中重要的一环，更是开源软件工作流程中不可或缺的一环。</p><p>既然要学习 Issue 的管理，那么就要回答以下问题：</p><ol><li>Issue 有什么用？为什么要进行 Issue 管理？</li><li>有哪些种类的 Issue？各种 Issue 一般又有什么样的模版？</li><li>何时应该创建一个 Issue？关闭一个 Issue 呢？哪些信息又不应该放进 Issue？</li><li>如何才能持续收到 Issue 页面的更新？</li><li>（进阶，本次不准备回答）对开源软件，作为用户/普通贡献者/仓库管理者分别有什么样的 Issue 管理原则和注意事项？</li></ol><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><blockquote><p>解决办法适用的环境/版本</p></blockquote><p>github、gitlab 都有内建的 Issue 功能。但是 Issue Track System 的范畴比这些例子要广一些，是一类软件系统的统称。</p><p>Issue Track System 在某些方面类似 Bug Tracker，wikipedia 上有个 <a href="https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems" target="_blank" rel="noopener">issue tracking system 的比较</a>。</p><h2 id="问题和回答"><a href="#问题和回答" class="headerlink" title="问题和回答"></a>问题和回答</h2><blockquote><p>详细描述问题的范畴/条件/上下文/…<br>【回答的可操作性】逐步说明用什么工具, 在哪儿, 进行什么操作, 如何检验, 应该获得什么输出 …</p></blockquote><h3 id="Issue-有什么用？为什么要进行-Issue-管理？"><a href="#Issue-有什么用？为什么要进行-Issue-管理？" class="headerlink" title="Issue 有什么用？为什么要进行 Issue 管理？"></a>Issue 有什么用？为什么要进行 Issue 管理？</h3><p>Issue 是团队内/团队和外部用户之间交流的一个渠道，直接和某个代码库相关联。同时采取「公告板」形式，和不分主题的「聊天记录」形式相对，一个帖子的讨论历史集中，便于查找。</p><p>因此 Issue 的适用条件：</p><ul><li>以某个代码库为中心的讨论；</li><li>文本量较大，更接近电邮、文章而非聊天的讨论；</li><li>可能需要持续很长时间（数天、数月）的讨论；</li><li>同一主题下回复历史中的信息非常重要的讨论；</li><li>记录需要长期保存以备查找的讨论。</li></ul><p>Issue 是「信息流模式」——和以逻辑联系的「wiki模式」或者说「书籍模式」相对。不断有新的 Issue Open，生命周期结束的 Issue Close，Issue 的页面的经典阅读模式也是从上到下的流模式，外加检索和筛选功能。</p><p>因此：</p><ul><li>每种 Issue 都必须有其生命周期，必须在某个时刻关闭，因为首页的空间资源和团队的注意力资源都是有限的。</li><li>同时 Issue 作为一个「流模式」讨论版，除了记录关闭后的归档之外，不应该承担任何知识库的功能。<ul><li>任何在关闭后有用的信息都应该整理归档到其他地方。例如：</li><li>「常见问题」在关闭后应该归档到项目 wiki 专门的 FAQ 收集页面（虽然现在还不存在）；</li><li>「工具指南」应该归档到团队的知识库，在蟒营也是项目 wiki 的知识库页面。</li></ul></li></ul><p>重申一遍原则：</p><ol><li>Issue 不承担知识库的功能，任何完善到可用的 Issue 都应该及时整理归档，保持 Issue 页面整洁。</li><li>（几乎）任何 Issue 都需要在某个时刻关闭。</li></ol><h3 id="有哪些种类的-Issue？创建-Issue-又有什么样的要求和模版？"><a href="#有哪些种类的-Issue？创建-Issue-又有什么样的要求和模版？" class="headerlink" title="有哪些种类的 Issue？创建 Issue 又有什么样的要求和模版？"></a>有哪些种类的 Issue？创建 Issue 又有什么样的要求和模版？</h3><p>普适的我没精力去整理。本课程的学习中出现了如下几种 Issue，所有模版在 Issue 页面点击 <a href="https://gitlab.com/101camp/2py/tasks/issues/new?issue%5Bassignee_id%5D=&issue%5Bmilestone_id%5D=" target="_blank" rel="noopener">New Issue</a> 按钮可以看到。</p><p>对一个项目，常见的 Issue 最好要有模版，可以复用。</p><ul><li>Ask 模版：日常提问模板。</li><li>Report 模版：学员周任务汇报模版，标题格式——[任务周] &lt;学员ID&gt; (心情断言)</li><li>Handbook 模版，tag [FAQ]：日常手册/指北 分享。</li></ul><p><strong>[FAQ] tag 滥用的问题</strong></p><p>目前大家似乎在滥用 [FAQ] tag，普通的提问并不值得成为所谓的”Frequent Asked Question”。一是不一定 Frequent，二是普通提问的可阅读性、知识密度往往很难有「手册」级别的价值。</p><p>信息爆炸成一团噪音，信息则无价值。[FAQ] 这个 tag 应该是对那些作者努力收集了所有可用信息，改善了文章的可读性、泛用性，同时又真正能帮到许多人的文章的标志。换言之标[FAQ]应该是作者对内容质量的一个承诺。</p><h3 id="何时应该创建一个-Issue？"><a href="#何时应该创建一个-Issue？" class="headerlink" title="何时应该创建一个 Issue？"></a>何时应该创建一个 Issue？</h3><p>模版的类型基本可以回答大部分「何时创建」的问题了。「信息过载」的度请各位自行判断。</p><p>同时对模版覆盖不了的情况，再次重申 Issue 的适用条件：</p><ul><li>以某个代码库为中心的讨论；</li><li>文本量较大，更接近电邮、文章而非聊天的讨论；</li><li>可能需要持续很长时间（数天、数月）的讨论；</li><li>同一主题下回复历史中的信息非常重要的讨论；</li><li>记录需要长期保存以备查找的讨论。</li></ul><p>有时有可能是模版还没 cover 到，所以各位可以适时地自己创造模版，然后发布在社区中。</p><h3 id="何时关闭一个-Issue-？"><a href="#何时关闭一个-Issue-？" class="headerlink" title="何时关闭一个 Issue ？"></a>何时关闭一个 Issue ？</h3><p>Issue 一定需要关闭。「关闭」意味着 Issue 不会出现在 Open 栏，相当于被暂时归档。</p><p>关闭的 Issue 仍然可以被查找，只是相当于不出现在首页上，不占用大家宝贵的注意力资源。同时关闭了的 Issue 也可以重新被打开。</p><p>各种类型 Issue 的推定关闭时机</p><ul><li>提问 Issue——问题解答后关闭。视热度选择是否进一步处理，成为一篇文章。</li><li>FAQ Issue——文章完善后关闭，归档到对应的知识库。</li><li>特殊功能型 Issue——完成功能立刻关闭。例如“Hello World” Issue。</li><li>需要他人确认、合作的Issue——完成功能后关闭。例如周任务报告，可能需要老师确认。</li></ul><h3 id="哪些信息又不应该放进-Issue？"><a href="#哪些信息又不应该放进-Issue？" class="headerlink" title="哪些信息又不应该放进 Issue？"></a>哪些信息又不应该放进 Issue？</h3><p>永远的: <a href="https://gitlab.com/101camp/2py/tasks/wikis/HandBooks/Hb4Ask" target="_blank" rel="noopener">如何提问</a></p><h3 id="如何才能持续收到-Issue-页面的更新？"><a href="#如何才能持续收到-Issue-页面的更新？" class="headerlink" title="如何才能持续收到 Issue 页面的更新？"></a>如何才能持续收到 Issue 页面的更新？</h3><p>gitlab 的 Issue 页面有 <a href="https://gitlab.com/101camp/2py/tasks/issues.atom?feed_token=kHJS1J-xZ5reczvJ89eh&state=opened" target="_blank" rel="noopener">RSS feed</a>。</p><p>用 RSS 接收 Issue 更新似乎是业界的普遍实践。如果大家有不了解 RSS 的，可以参考这一篇 <a href="https://sspai.com/post/54658" target="_blank" rel="noopener">中文文章</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p>再次说明经验应用要注意的, 容易出问题的点, 以及有助记忆的作弊条…</p></blockquote><ol><li>Issue 不承担知识库的功能，任何完善到可用的 Issue 都应该及时整理归档，保持 Issue 页面整洁。</li><li>（几乎）任何 Issue 都需要在某个时刻关闭。</li><li>对 [FAQ] tag 的滥用：<ul><li>[FAQ] 这个 tag 应该是对那些作者努力收集了所有可用信息，改善了文章的可读性、泛用性，同时又真正能帮到许多人的文章的标志。换言之标[FAQ]应该是作者对内容质量的一个承诺。</li></ul></li><li>各种类型 Issue 的推定关闭时机<ul><li>提问 Issue——问题解答后关闭。视热度选择是否进一步处理，成为一篇文章。</li><li>FAQ Issue——文章完善后关闭，归档到对应的知识库。</li><li>特殊功能型 Issue——完成功能立刻关闭。例如“Hello World” Issue。</li><li>需要他人确认、合作的Issue——完成功能后关闭。例如周任务报告，可能需要老师确认。</li></ul></li></ol><h2 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h2><blockquote><p>过程中参考过的重要文章/图书/模块/代码/…</p></blockquote><ul><li><a href="https://en.wikipedia.org/wiki/Issue_tracking_system#cite_note-2" target="_blank" rel="noopener">Issue tracking system</a></li><li><a href="https://www.projectmanager.com/training/managing-project-issues" target="_blank" rel="noopener">7 Steps for Managing Project Issues</a></li><li>永远的: <a href="https://gitlab.com/101camp/2py/tasks/wikis/HandBooks/Hb4Ask" target="_blank" rel="noopener">如何提问</a></li></ul><h2 id="logging"><a href="#logging" class="headerlink" title="logging:"></a>logging:</h2><blockquote><p>用倒序日期排列来从旧到新记要关键变化</p><ul><li>190716 init</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Consistent use of an issue or bug tracking system is considered one of the “hallmarks of a good software team”. – wikipedia&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作为社会化软件开发的必备技能，本文分享一些 Issue 管理的常识。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://yoursite.com/categories/Programming/"/>
    
      <category term="Social Development" scheme="http://yoursite.com/categories/Programming/Social-Development/"/>
    
    
      <category term="lan-chinese" scheme="http://yoursite.com/tags/lan-chinese/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
      <category term="gitlab" scheme="http://yoursite.com/tags/gitlab/"/>
    
      <category term="camp2py" scheme="http://yoursite.com/tags/camp2py/"/>
    
  </entry>
  
  <entry>
    <title>Mac Custom</title>
    <link href="http://yoursite.com/2019/07/15/mac-custom/"/>
    <id>http://yoursite.com/2019/07/15/mac-custom/</id>
    <published>2019-07-15T08:09:25.000Z</published>
    <updated>2019-07-17T12:35:44.487Z</updated>
    
    <content type="html"><![CDATA[<p>Mac 电脑整体的设计思路和机甲片里的「特装机」一样——不惜工本，提升某几个方面的性能，不易整备，零件/技能不易通用。</p><a id="more"></a><p>但 Mac 好的是在特化的同时保持了界面友好性，学习成本相当低，比起难以驾驭的「古铁巨人」更像界面友好的「初代高达」——并不需要太多特殊技术也能发挥相当一部分性能，单纯只是造价高昂、不易维护而已。</p><p>特机给一般操纵士来开，当然会有诸多怨言——难以后勤补给、和队友的兼容性差、某些方面因为特殊设计而产生缺陷。同时对特机的特化性能优势，如果抱着一般操纵士的思维，也难以有效利用。</p><p>当然也有「相良宗介」这种量产大神……但是人家嘴上百般嫌弃特机「强弩」，最后还是乖乖一路用「强弩」和「利维坦」打到了破关:) </p><p>特机是设计出来在特殊情境下发挥非凡价值的机械，其能力的发挥需要配得上它的特殊操纵士和合适的场景。一般操纵士开特机会有反效果，是因为他们承受了特机带来的问题，却没能发挥出特机的价值。</p><blockquote><p>Macintosh、出る！</p></blockquote><p>另外HHKB是真的需要一些特殊技巧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Mac 电脑整体的设计思路和机甲片里的「特装机」一样——不惜工本，提升某几个方面的性能，不易整备，零件/技能不易通用。&lt;/p&gt;
    
    </summary>
    
      <category term="Hardware" scheme="http://yoursite.com/categories/Hardware/"/>
    
      <category term="MacBook" scheme="http://yoursite.com/categories/Hardware/MacBook/"/>
    
    
      <category term="lan-chinese" scheme="http://yoursite.com/tags/lan-chinese/"/>
    
      <category term="productivity" scheme="http://yoursite.com/tags/productivity/"/>
    
      <category term="robot" scheme="http://yoursite.com/tags/robot/"/>
    
      <category term="design" scheme="http://yoursite.com/tags/design/"/>
    
  </entry>
  
  <entry>
    <title>Web Scraping Introduction with Scrapy</title>
    <link href="http://yoursite.com/2019/07/12/web-scraping-intro-with-scrapy/"/>
    <id>http://yoursite.com/2019/07/12/web-scraping-intro-with-scrapy/</id>
    <published>2019-07-12T15:27:41.000Z</published>
    <updated>2019-07-12T15:37:52.964Z</updated>
    
    <content type="html"><![CDATA[<p>Resources</p><ul><li><a href="https://www.datacamp.com/community/tutorials/making-web-crawlers-scrapy-python" target="_blank" rel="noopener">Making Web Crawlers Using Scrapy for Python</a></li><li><a href="https://docs.scrapy.org/en/latest/intro/overview.html" target="_blank" rel="noopener">docs of Scrapy</a></li><li><a href="https://github.com/geekan/scrapy-examples/tree/master/doubanbook" target="_blank" rel="noopener">geekan/scrapy-examples</a></li></ul><a id="more"></a><h2 id="Create-a-project"><a href="#Create-a-project" class="headerlink" title="Create a project"></a>Create a project</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject &lt;names&gt;</span><br></pre></td></tr></table></figure><h2 id="First-spider"><a href="#First-spider" class="headerlink" title="First spider"></a>First spider</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure><h2 id="Run-spider"><a href="#Run-spider" class="headerlink" title="Run spider"></a>Run spider</h2><p>Go to top level and run:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure><p>Start url can also define explictly in variable <strong>start_urls</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">        <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure><h2 id="Extracting-data"><a href="#Extracting-data" class="headerlink" title="Extracting data"></a>Extracting data</h2><p>response.css() method can do this.</p><p>It returns a <strong>Selectlist</strong> object. Can be stripped and queried further.</p><p><strong>Methods</strong></p><ul><li>getall()</li><li>get()</li><li>re(): Extract using regular expressions.</li></ul><p>response.xpath() method is more under-the-hood.</p><p>A example below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">            <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">            <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="Store-the-scraped-data"><a href="#Store-the-scraped-data" class="headerlink" title="Store the scraped data"></a>Store the scraped data</h2><blockquote><p>For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a broken JSON file.</p></blockquote><p>The JSON lines format is easy to append.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.json</span><br><span class="line">scrapy crawl quotes -o quotes.jl  <span class="comment"># JSON lines</span></span><br></pre></td></tr></table></figure><p>All formats for <a href="https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="noopener">Feed exports</a></p><h2 id="Follow-links"><a href="#Follow-links" class="headerlink" title="Follow links"></a>Follow links</h2><p>With attrib[] attribute, or select in the css() method.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line"><span class="string">'/page/2/'</span></span><br><span class="line">&gt;&gt;&gt; response.css(<span class="string">'li.next a'</span>).attrib[<span class="string">'href'</span>]</span><br><span class="line"><span class="string">'/page/2'</span></span><br></pre></td></tr></table></figure><p><strong>Recursively follow the “Next Page” link</strong></p><p>Build the full absolute URL with urljoin() method.</p><blockquote><p>What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">        yield &#123;</span><br><span class="line">            <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).get(),</span><br><span class="line">            <span class="string">'author'</span>: quote.css(<span class="string">'small.author::text'</span>).get(),</span><br><span class="line">            <span class="string">'tags'</span>: quote.css(<span class="string">'div.tags a.tag::text'</span>).getall(),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    next_page = response.css(<span class="string">'li.next a::attr(href)'</span>).get()</span><br><span class="line">    <span class="keyword">if</span> next_page is not None:</span><br><span class="line">        next_page = response.urljoin(next_page)</span><br><span class="line">        yield scrapy.Request(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure><p>response.follow support relative URLs directly. Otherwise you can pass a selector directly(with necessary attributes).</p><p>For &lt;a&gt; method there’s a shortcut: follow their href attribute automatically.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> next_page is not None:</span><br><span class="line">    yield response.follow(next_page, callback=self.parse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pass a selector</span></span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">    yield response.follow(href, callback=self.parse)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># follow &lt;a&gt; elements directly</span></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> response.css(<span class="string">'li.next a'</span>):</span><br><span class="line">    yield response.follow(a, callback=self.parse)</span><br></pre></td></tr></table></figure><h2 id="For-Deeper-Scraping"><a href="#For-Deeper-Scraping" class="headerlink" title="For Deeper Scraping"></a>For Deeper Scraping</h2><p>With the callback variable of response.follow() or scrapy.Request() method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthorSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'author'</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># follow links to author pages</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'.author + a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse_author)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># follow pagination links</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'li.next a::attr(href)'</span>):</span><br><span class="line">            <span class="keyword">yield</span> response.follow(href, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_author</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">extract_with_css</span><span class="params">(query)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">''</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'name'</span>: extract_with_css(<span class="string">'h3.author-title::text'</span>),</span><br><span class="line">            <span class="string">'birthdate'</span>: extract_with_css(<span class="string">'.author-born-date::text'</span>),</span><br><span class="line">            <span class="string">'bio'</span>: extract_with_css(<span class="string">'.author-description::text'</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>Q: How can I pass some information of current page to the next page to be parsed?</p><h2 id="Arguments"><a href="#Arguments" class="headerlink" title="Arguments"></a>Arguments</h2><p>Spiders can have <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#spiderargs" target="_blank" rel="noopener">arguments</a>.</p><h2 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h2><blockquote><p>After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.</p></blockquote><p><a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">more information</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Resources&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.datacamp.com/community/tutorials/making-web-crawlers-scrapy-python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Making Web Crawlers Using Scrapy for Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.scrapy.org/en/latest/intro/overview.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docs of Scrapy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/geekan/scrapy-examples/tree/master/doubanbook&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;geekan/scrapy-examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Programming" scheme="http://yoursite.com/categories/Programming/"/>
    
      <category term="Web Crawler" scheme="http://yoursite.com/categories/Programming/Web-Crawler/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
      <category term="lan-english" scheme="http://yoursite.com/tags/lan-english/"/>
    
      <category term="web-scraping" scheme="http://yoursite.com/tags/web-scraping/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/07/12/hello-world/"/>
    <id>http://yoursite.com/2019/07/12/hello-world/</id>
    <published>2019-07-12T14:45:42.001Z</published>
    <updated>2019-07-12T16:08:19.229Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="life" scheme="http://yoursite.com/categories/life/"/>
    
    
  </entry>
  
</feed>
